#!/usr/bin/env bash
set -e

echo "ðŸš€ Uruchamianie Å›rodowiska danych (Hadoop + Kafka + Mongo + Spark + NiFi)..."

# ========== KONFIGURACJA ==========
NETWORK_NAME="data-net"
HADOOP_COMPOSE="./hadoop/docker-compose.yaml"
KAFKA_COMPOSE="./kafka/docker-compose.yaml"
MONGO_COMPOSE="./mongo/docker-compose.yaml"
SPARK_COMPOSE="./spark/docker-compose.yaml"
NIFI_COMPOSE="./nifi/docker-compose.yaml"
JUPYTER_COMPOSE="./jupyter/docker-compose.yaml"

# ========== TWORZENIE SIECI ==========
echo ""
echo "ðŸ”¹ Tworzenie sieci Docker: $NETWORK_NAME"
docker network inspect "$NETWORK_NAME" >/dev/null 2>&1 || docker network create "$NETWORK_NAME"

# ========== URUCHAMIANIE HADOOP ==========
echo ""
echo "ðŸ”¹ Uruchamianie Hadoop..."
docker compose -f "$HADOOP_COMPOSE" up -d

# czekamy aÅ¼ namenode siÄ™ podniesie
echo "â³ Czekam aÅ¼ NameNode wystartuje (ok. 20s)..."
sleep 20
docker exec namenode hdfs dfs -mkdir -p /user/hadoop || true

# ========== URUCHAMIANIE KAFKA ==========
echo ""
echo "ðŸ”¹ Uruchamianie Kafka..."
docker compose -f "$KAFKA_COMPOSE" up -d

# czekamy aÅ¼ broker wystartuje
echo "â³ Czekam aÅ¼ broker Kafka siÄ™ uruchomi (ok. 10s)..."
sleep 10

# ========== URUCHAMIANIE MONGO ==========
echo ""
echo "ðŸ”¹ Uruchamianie MongoDB..."
docker compose -f "$MONGO_COMPOSE" up -d

# ========== URUCHAMIANIE SPARK ==========
echo ""
echo "ðŸ”¹ Uruchamianie Spark..."
docker compose -f "$SPARK_COMPOSE" up -d

# ========== URUCHAMIANIE NIFI ==========
echo ""
echo "ðŸ”¹ Uruchamianie NiFi..."
docker compose -f "$NIFI_COMPOSE" up -d
sleep 20
echo ""
echo "ðŸ”¥ User i password do NiFi znajdziesz w logach kontenera:"
docker logs nifi | grep Generated

# ========== TESTY ==========
echo ""
echo "âœ… Test poÅ‚Ä…czenia w sieci (namenode, broker, mongo)"
docker run --rm --network "$NETWORK_NAME" alpine sh -c "
  apk add --no-cache curl bind-tools >/dev/null
  echo 'Ping namenode:' && ping -c1 namenode
  echo 'Ping broker:' && ping -c1 broker
  echo 'Ping mongo:' && ping -c1 mongo
"

echo ""
echo "âœ… Test HDFS:"
docker exec namenode hdfs dfs -ls /

echo ""
echo "âœ… Test Kafka (lista topikÃ³w):"
docker exec broker bash -c "/opt/kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --list || true"

# ========== URUCHAMIANIE JUPYTER LAB ==========
echo ""
echo "ðŸ”¹ Uruchamianie Jupyter Lab..."
docker compose -f "$JUPYTER_COMPOSE" up -d

# ========== STATUS ==========
echo ""
echo "ðŸ“Š Status kontenerÃ³w:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo ""
echo "âœ… Wszystkie komponenty powinny byÄ‡ dostÄ™pne:"
echo " â€¢ Jupyter Lab:    â†’ http://localhost:8888"
echo " â€¢ Hadoop UI:      â†’ http://localhost:9870"
echo " â€¢ YARN UI:        â†’ http://localhost:8088"
echo " â€¢ Spark Master UI â†’ http://localhost:8080"
echo " â€¢ Spark Worker UI â†’ http://localhost:8081"
echo " â€¢ Kafka broker:   â†’ broker:9092"
echo " â€¢ Mongo UI:       â†’ http://localhost:8082 (admin / pass)"
echo " â€¢ NiFi UI:        â†’ https://localhost:8443 (login admin / hasÅ‚o w logach kontenera)"
echo ""
echo ""
echo "ðŸ”¥ Aby zatrzymaÄ‡ wszystkie serwisy:"
echo "   ./stop-data-stack.sh"
echo ""
echo "ðŸ”„ Aby zatrzymaÄ‡ pojedyncze serwisy:"
echo "   docker compose -f $HADOOP_COMPOSE down"
echo "   docker compose -f $KAFKA_COMPOSE down"
echo "   docker compose -f $MONGO_COMPOSE down"
echo "   docker compose -f $SPARK_COMPOSE down"
echo "   docker compose -f $NIFI_COMPOSE down"
